<?xml version="1.0" encoding="UTF-8"?>
<configuration debug="false" scan="true" scanPeriod="30 minutes">
	<contextName>logback</contextName>

	<!-- 打印到控制台 -->
	<appender name="stdout" class="ch.qos.logback.core.ConsoleAppender">
		<encoder>
			<pattern>%date{yyyy-MM-dd HH:mm:ss.SSS} [%thread] %-5level-%logger{5} - %msg%n</pattern>
		</encoder>
		<!--定义了一个过滤器,在LEVEL之下的日志输出不会被打印出来-->
		<!--这里定义了DEBUG，也就是控制台不会输出比ERROR级别小的日志-->
		<!--        <filter class="ch.qos.logback.classic.filter.ThresholdFilter">-->
		<!--            <level>INFO</level>-->
		<!--        </filter>-->
	</appender>

	<appender name="file" class="ch.qos.logback.core.rolling.RollingFileAppender">
		<!--定义日志输出的路径-->
		<!--这里的scheduler.manager.server.home 没有在上面的配置中设定，所以会使用java启动时配置的值-->
		<!--比如通过 java -Dscheduler.manager.server.home=/path/to XXXX 配置该属性-->
		<file>logs/last.log</file>
		<!--定义日志滚动的策略-->
		<rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy">
			<!--定义文件滚动时的文件名的格式-->
			<fileNamePattern>logs/log-%d{yyyy-MM-dd}.%i.log.gz</fileNamePattern>
			<!--30天的时间周期，之前的将被清理掉-->
			<maxHistory>30</maxHistory>
			<!--日志总保存量为10GB-->
			<totalSizeCap>10GB</totalSizeCap>
			<timeBasedFileNamingAndTriggeringPolicy class="ch.qos.logback.core.rolling.SizeAndTimeBasedFNATP">
				<!--文件达到 最大50MB时会被压缩和切割 -->
				<maxFileSize>50MB</maxFileSize>
			</timeBasedFileNamingAndTriggeringPolicy>
		</rollingPolicy>
		<filter class="ch.qos.logback.classic.filter.ThresholdFilter">
			<level>INFO</level>
		</filter>
		<encoder>
			<pattern>%d [%thread] %-5level %logger{36} [%file : %line] - %msg%n</pattern>
			<charset>UTF-8</charset>
		</encoder>
	</appender>

	<!--屏蔽某些包中的日志输出-->
	<logger name="org.apache.kafka" level="off"/>
	<root level="info">
		<appender-ref ref="stdout"/>
		<appender-ref ref="file"/>
	</root>

</configuration>
